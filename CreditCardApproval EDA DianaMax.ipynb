{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02140434-d629-4646-930e-d4509d20036a",
   "metadata": {},
   "source": [
    "# CreditCardApproval EDA Diana Max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ea79c-4637-495f-8a7f-c6f58ccbd03e",
   "metadata": {},
   "source": [
    "## Setup notebook & fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e6793-95ae-402c-a1b5-4628c28f280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "#from ydata_profiling import ProfileReport\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.contingency import association\n",
    "from scipy.stats import pointbiserialr, pearsonr, spearmanr\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import TargetEncoder, FunctionTransformer, LabelBinarizer, label_binarize, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63854aca-03da-47ad-83e0-dc3006d92d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_approval = fetch_ucirepo(id=27)\n",
    "\n",
    "X = credit_approval.data.features\n",
    "y = credit_approval.data.targets\n",
    "df = credit_approval.data.original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7140128-8f93-45dd-89fa-ff536f3a2c65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## First look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19ac2c6-0fe1-43ba-bcce-be7066111907",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_approval.data.original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6918b-147b-4827-a88f-e0864ef58ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a99cf-9f43-4b59-a9d0-92c8841536b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Univariate EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452726a4-82f2-42c0-98c6-2413814c73b8",
   "metadata": {},
   "source": [
    "### A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32331065-35fc-42bb-a1fc-dac09ef88081",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = df[['A1','A16']]\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=A1.reset_index().groupby(['A1','A16']).count().reset_index(),\n",
    "    x='A1',\n",
    "    y='index',\n",
    "    hue='A16')\n",
    "ax.set(ylabel='value count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f16a1fb-fbcb-437d-aea0-559ae6420932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4081965-1278-42d7-9e02-5949e5bbeb0b",
   "metadata": {},
   "source": [
    "Impute missing values with most frequent category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d283c-310e-4fa3-af88-cc381a447767",
   "metadata": {},
   "source": [
    "### A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b7ad8-492e-48ff-8c71-738ae1e7c571",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = df[['A2','A16']]\n",
    "sns.histplot(A2, x='A2', hue='A16',stat='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c001daa4-96a8-47f8-8047-29bda2a64db6",
   "metadata": {},
   "source": [
    "Let's try a log1p transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a67126a-2a30-418e-9ef0-f345f63c28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = A2.assign(A2=A2.A2.apply(np.log1p))\n",
    "sns.histplot(A2, x='A2', hue='A16',stat='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d187e5-2983-425c-8fb0-d9f4a0b9bb5a",
   "metadata": {},
   "source": [
    "The distribution is less skewed now, so it seems like a usefull transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6f199-e456-43ad-a40a-e26713d3d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A2.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bfe46-ab94-4740-8214-df2b73b5b7bc",
   "metadata": {},
   "source": [
    "Its still skewed, so impute missing values with median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40dd30-41ca-44ca-a36a-b3d84fc72c33",
   "metadata": {},
   "source": [
    "### A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a1447f-af9b-496f-914b-b0fa72049065",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3 = df[['A3','A16']]\n",
    "sns.histplot(A3, x='A3', hue='A16',stat='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9153f1ff-b0a0-4e28-8b0c-eaf11675fea4",
   "metadata": {},
   "source": [
    "Let's try a log1p transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ad311-8215-44a1-af39-a1c43290bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3 = A3.assign(A3=A3.A3.apply(np.log1p))\n",
    "sns.histplot(A3, x='A3', hue='A16',stat='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225fc9f4-3010-44ea-b30c-a2bef0af41ca",
   "metadata": {},
   "source": [
    "The distribution is much less skewed now. Looks like a very good option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb531ed-8813-4c70-b8bc-c7bd569cf8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A3.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3980534d-1c97-4258-9286-3002ab864fd1",
   "metadata": {},
   "source": [
    "Its still skewed, so impute missing values with median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a77ea-ac33-40b6-80f7-9b3bd4a63a08",
   "metadata": {},
   "source": [
    "### A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cecb5f0-ea49-4469-8779-f405d182b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4 = df[['A4','A16']]\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=A4.reset_index().groupby(['A4','A16']).count().reset_index(),\n",
    "    x='A4',\n",
    "    y='index',\n",
    "    hue='A16')\n",
    "ax.set(ylabel='value count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e02828",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4['A4'] = A4['A4'].replace('l', 'u')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c027730",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(\n",
    "    data=A4.reset_index().groupby(['A4','A16']).count().reset_index(),\n",
    "    x='A4',\n",
    "    y='index',\n",
    "    hue='A16')\n",
    "ax.set(ylabel='value count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d6719-df1a-4231-ad47-c640c67d1d4e",
   "metadata": {},
   "source": [
    "Bin categories to u and non-u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5c0e7-0daa-4430-a0cb-ac5aff4601a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A4.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5607604-30e2-48a3-b2bc-2a23576f3542",
   "metadata": {},
   "source": [
    "Replace missing values with the most-frequent category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa00725-2326-442d-b555-3891eb4d9f48",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c1cfac-09ea-4fbe-bf0a-4bb51e868e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5 = df[['A5','A16']]\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=A5.reset_index().groupby(['A5','A16']).count().reset_index(),\n",
    "    x='A5',\n",
    "    y='index',\n",
    "    hue='A16')\n",
    "ax.set(ylabel='value count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a046d50-b105-4308-aa51-653ddef0ca18",
   "metadata": {},
   "source": [
    "Bin categories to g and non-g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab595ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "A5['A5'] = A5['A5'].replace(['gg', 'p'], 'non-g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(\n",
    "    data=A5.reset_index().groupby(['A5','A16']).count().reset_index(),\n",
    "    x='A5',\n",
    "    y='index',\n",
    "    hue='A16')\n",
    "ax.set(ylabel='value count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15027f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A5.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d489cb-965d-4093-a073-8fde52fe2445",
   "metadata": {},
   "source": [
    "Replace missing values with the most-frequent category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f420628-a28b-46c5-93b8-6acef6215a1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17866573-4c92-478e-8d88-937b8f7fd1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(\n",
    "    data=df.reset_index().groupby(['A6','A16']).count().reset_index(),\n",
    "    x='A6',\n",
    "    y='index',\n",
    "    hue='A16')\n",
    "ax.set(ylabel='value count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3f504-935f-40d3-9dce-8b9761e70585",
   "metadata": {},
   "source": [
    "Many categories with quite a range of different distributions of A16 (target) values. Target encoding seems like a reasonable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d052e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_pipeline(TargetEncoder(), SimpleImputer(strategy=\"median\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30c9bb-362f-404d-95a9-12f28f8c47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(\n",
    "    data= pd.DataFrame({\n",
    "        'A6': TargetEncoder().fit_transform(df.A6.values[:,np.newaxis], df.A16).ravel(),\n",
    "        'A16': df.A16.values}),\n",
    "    x='A6',\n",
    "    hue='A16',\n",
    "    stat='density'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235266b3-c26b-4f6f-bcd2-0cf59c499836",
   "metadata": {},
   "source": [
    "Target encoding looks promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f2a95-8083-4de4-9041-b9bd2e7d3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A6.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed0ef5-746b-4644-b5a5-27e365a54733",
   "metadata": {},
   "source": [
    "Impute with median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1f6b4-71af-4af6-8dd7-7f43fcae806b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c5b43-1492-4075-a226-94eb314753d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(\n",
    "    data=df.reset_index().groupby(['A7','A16']).count().reset_index(),\n",
    "    x='A7',\n",
    "    y='index',\n",
    "    hue='A16')\n",
    "ax.set(ylabel='value count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a700f626-d805-4f55-8de1-e75cdd74b1af",
   "metadata": {},
   "source": [
    "Naively try target encoding first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf1454-335d-40ce-a3e4-d9f5866f2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(\n",
    "    data= pd.DataFrame({\n",
    "        'A7': TargetEncoder().fit_transform(df.A7.values[:,np.newaxis], df.A16).ravel(),\n",
    "        'A16': df.A16.values}),\n",
    "    x='A7',\n",
    "    hue='A16',\n",
    "    kde=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff847664-0bf6-48c2-bc5f-f4baab539da2",
   "metadata": {},
   "source": [
    "This looks not quite usefull yet, as the kernel density estimates show curves of almost the same shape and location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053dbb34-97be-4aa3-b9e0-ea9e4e6a75cc",
   "metadata": {},
   "source": [
    "Many categories with almost no values. Summarising those categories, that have similar distributions of A16, so: (v, dd, j), (h, z), (bb, n, o), (ff), could be an option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac791a5-9f4a-4134-8485-13f88e8f7e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(\n",
    "    data=pd.DataFrame({\n",
    "        'A7 new': df.A7.replace({'ff':'not v','dd':'not v','j':'not v', 'h':'not v','z':'not v', 'bb':'not v','n':'not v','o':'not v'}).ravel(),\n",
    "        'A16': df.A16.values}),\n",
    "    x='A7 new',\n",
    "    hue='A16',\n",
    "    multiple='dodge',\n",
    "    shrink=.8\n",
    "    #stat='density'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae43769-6948-41b4-930e-a305c06d6998",
   "metadata": {},
   "source": [
    "This still doesnt look very usefull, suggesting either the features is generally bad (at least for a linear classifier) or the new categories are bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d067509-49d5-4a36-bb18-15ff0bfccfc0",
   "metadata": {},
   "source": [
    "We will come back to this feature later on. We will have a look at correlations and mutual information first to get a little more information about this feature. If it contians high information, OneHot encoding might be worth it (as it adds dimensions but this might be worth for a good feature).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c6d9d-28a2-4831-9419-3fedc145c524",
   "metadata": {},
   "source": [
    "Otherwise we might try other binnings of categories or throw the feature out all toghether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b43b2a-17fe-43ae-9947-43f9021cbb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A7.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747b48f-08c2-457a-8295-b0263bf917b3",
   "metadata": {},
   "source": [
    "Impute missing values with most frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1423984a-0c2f-474f-a68e-757685bff551",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed74f76-948a-48eb-873a-514c1117f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A8 = df[['A8','A16']]\n",
    "sns.histplot(A8, x='A8', hue='A16',stat='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f830126-7fd3-4869-8fbc-11a14637da4f",
   "metadata": {},
   "source": [
    "Try a log1p transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8e6a1-5137-4478-b75e-c4d949393fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A8 = A8.assign(A8=A8.A8.apply(np.log1p))\n",
    "sns.histplot(A8, x='A8', hue='A16',stat='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3361a824-09d6-48cb-8c71-cfcc695ede56",
   "metadata": {},
   "source": [
    "Although this already looks quite promising a common logarithm (base = 10) might be even better, as the distribution is still strongly left skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e6ee8-e15d-4072-96c6-ef31f2740296",
   "metadata": {},
   "outputs": [],
   "source": [
    "A8 = A8.assign(A8=A8.A8.apply(np.log10))\n",
    "sns.histplot(A8, x='A8', hue='A16',stat='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b9f1f9-fafe-416e-9455-908a43a7a4f0",
   "metadata": {},
   "source": [
    "Looks reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d7730-5ad0-48a4-b129-6a5918b50b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A8.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6376821-bd40-49f0-a17e-57c4e73b5aa1",
   "metadata": {},
   "source": [
    "No missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f150f-8394-4b10-9159-e495067a6653",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc432e0c-0bb5-422b-8124-4ab76d847e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "A9 = df[['A9','A16']]\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=A9.reset_index().groupby(['A9','A16']).count().reset_index(),\n",
    "    x='A9',\n",
    "    y='index',\n",
    "    hue='A16')\n",
    "ax.set(ylabel='value count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5acf40-2365-4cdc-b462-54c1dd82ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A9.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e1056c-eb77-42f4-b34b-f7163a9a8221",
   "metadata": {},
   "source": [
    "Feature is already in binary format and doesn't have any missing values. Perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0043428-9cd1-4ad2-89bc-0d431a251e34",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b9ad7-ee1b-4a89-878f-73dc4b949d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "A10 = df[['A10','A16']]\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=A10.reset_index().groupby(['A10','A16']).count().reset_index(),\n",
    "    x='A10',\n",
    "    y='index',\n",
    "    hue='A16')\n",
    "ax.set(ylabel='value count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3136f943-8b5e-4360-9a81-812519281864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A10.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d53562a-9ee4-49d2-89cf-44f6961c6b1a",
   "metadata": {},
   "source": [
    "Feature is already in binary format and doesn't have any missing values. Perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfac9cf-3c34-4bf5-99e5-fdae10af3239",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8e8659-5ae2-4642-b0c6-7ca607471472",
   "metadata": {},
   "outputs": [],
   "source": [
    "A11 = df[['A11','A16']]\n",
    "sns.histplot(A11, x='A11', hue='A16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d130a48-a658-4ac3-bea4-863d78e2e53c",
   "metadata": {},
   "source": [
    "Extremely strong left-skewed distribution with mostly Zeros. Apply a log1p transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec6cb1-6b95-4538-b0a7-5cf5e7101ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A11 = A11.assign(A11=A11.A11.apply(np.log1p))\n",
    "sns.histplot(A11, x='A11', hue='A16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89326200-4584-4896-99de-b643c0c70412",
   "metadata": {},
   "source": [
    "This looks much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380c76d-8c65-4073-8107-e5a0a3eb8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A11.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6376f2-798d-4cfa-b2f2-422a1bb9dc61",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85f53d-13ba-4040-92d3-b10559a9449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A12 = df[['A12','A16']]\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=A12.reset_index().groupby(['A12','A16']).count().reset_index(),\n",
    "    x='A12',\n",
    "    y='index',\n",
    "    hue='A16')\n",
    "ax.set(ylabel='value count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b757fe-70cc-49d4-a59b-12de6bcb99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A12.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc53f30-5da7-40db-ba7f-4885d9ccf6de",
   "metadata": {},
   "source": [
    "Feature is already in binary format and doesn't have any missing values. Perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788fd51-431c-4bac-9a80-4fff3807965b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521af4c-b450-40d5-bb9c-a908316824cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A13 = df[['A13','A16']]\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=A13.reset_index().groupby(['A13','A16']).count().reset_index(),\n",
    "    x='A13',\n",
    "    y='index',\n",
    "    hue='A16')\n",
    "ax.set(ylabel='value count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b4081-7bdc-4539-9ada-91a5009cf4c3",
   "metadata": {},
   "source": [
    "Binarise. Since the distirbution of A16 among p resembles more that of g, summarise g and p into one category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431af34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A13['A13'] = A13['A13'].replace(['s', 'p'], 'non-g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47989e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(\n",
    "    data=A13.reset_index().groupby(['A13','A16']).count().reset_index(),\n",
    "    x='A13',\n",
    "    y='index',\n",
    "    hue='A16')\n",
    "ax.set(ylabel='value count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0bb1be-abd9-491f-b45e-f407ac818c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A13.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301123e8-ff80-40a1-aefb-bde4d81f8e49",
   "metadata": {},
   "source": [
    "No missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad36a3d0-5a27-4402-a8f5-e3f828750d31",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af6dc4-db78-4ddf-9849-8cbbcca8cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A14 = df[['A14','A16']]\n",
    "sns.histplot(A14, x='A14', hue='A16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994e6ce-72bd-4a10-862b-3c6ee001a349",
   "metadata": {},
   "source": [
    "Strongly left-skewed, apply log1p transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29541770-ee7c-4a34-a0f1-861707b04a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "A14 = A14.assign(A14=A14.A14.apply(np.log1p))\n",
    "sns.histplot(A14, x='A14', hue='A16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29937cb-e83b-48e4-ad02-b01f1ba66b05",
   "metadata": {},
   "source": [
    "Fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e14981-ca5d-475b-a059-7b6826aab688",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A14.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88081bcd-9ca7-4119-95e2-dafe7634f8eb",
   "metadata": {},
   "source": [
    "Impute missing values with most frequent value, Zero, which is reasonably well balanced between values for the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e34e2b-b644-473a-9d5f-ad41ed01417c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c26ad-1545-4b27-8be2-6ddceaa04bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A15 = df[['A15','A16']]\n",
    "sns.histplot(A15, x='A15', hue='A16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff932738-b965-4dab-8390-33f0af25e4d9",
   "metadata": {},
   "source": [
    "Apply a log1p transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34251172-bbe3-4b33-aaff-ade652ec9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "A15 = A15.assign(A15=A15.A15.apply(np.log1p))\n",
    "sns.histplot(A15, x='A15', hue='A16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a48c5-437f-42a3-8807-ad39d52d6d9b",
   "metadata": {},
   "source": [
    "Looks much better. Keep that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af03f63-89a3-4cca-b7c2-beefb5d3a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A15.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17512ea9-fe18-49f9-8493-753a2d7c1fbe",
   "metadata": {},
   "source": [
    "No missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec5c85-f503-485a-910c-8546339b953d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0dcdd-7bc4-4aac-bd4a-db23c4afd1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A16.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c0cc1b-eb1a-4941-b3aa-7e526002bf33",
   "metadata": {},
   "source": [
    "## Multivariate EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a7b82-96b5-4be0-bcb8-781c7dade429",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NaN Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b1888-b37c-439d-9579-0ded1c81f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_approval.variables[['name','missing_values']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d284a86-7f7b-4367-b50b-77479c7a1815",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c9312-d5cc-40dc-9c48-26415a50060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any(axis=1).sum() / df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84034b1b-7668-4815-8623-8f93f847d457",
   "metadata": {},
   "source": [
    "About 5% of the data contains missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf3832-133a-4a10-a950-e65e0410e5e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "TODO: look if some features are always NaN at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959a08a-d95c-4dba-a202-0f22af842259",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Correlations with target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218b6963-b022-40a7-b91a-ff092b15766f",
   "metadata": {},
   "source": [
    "Correlate every variables with target:\n",
    "- categorical data: Cramers V\n",
    "- continuous data: Point biserial correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2855badf-7afe-41d4-8400-3e6c128af65a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f4a33-9165-41d9-9415-775239fca47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(15, 9), dpi=80)\n",
    "\n",
    "categorical_variables = credit_approval.variables[(credit_approval.variables.type == 'Categorical') & \n",
    "                                                  (credit_approval.variables.role == 'Feature')].name.values\n",
    "s = pd.Series(index=categorical_variables)\n",
    "\n",
    "\n",
    "for var in categorical_variables:\n",
    "    s[var] = association(pd.crosstab(df[var], df.A16))\n",
    "\n",
    "s.plot(kind='bar', title='Correlation with target \\n categorical data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba671c5d-d40e-4381-b70f-7c5acef034e1",
   "metadata": {},
   "source": [
    "#### continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bea63e7-5ebb-4927-8998-82a210e43805",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(15, 9), dpi=80)\n",
    "\n",
    "target = y.replace({'+':1,'-':0})\n",
    "\n",
    "continuous_variables = credit_approval.variables[(credit_approval.variables.type == 'Continuous') & (credit_approval.variables.role == 'Feature')].name.values\n",
    "\n",
    "s = pd.Series(index=continuous_variables)\n",
    "\n",
    "\n",
    "for var in continuous_variables:\n",
    "    s[var] = pointbiserialr(target[~df[var].isna()].values.ravel(), df[var].dropna().values).statistic\n",
    "\n",
    "s.plot(kind='bar', title='Correlation with target \\n continious data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413abf14-8ce6-44f0-ba65-becdd200f7b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Correlations and mutual information between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09865057-11f9-49d9-8ee7-ccbdc18d5f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute mutual information in every commbination of two variables. We don't use product-momment or rank correlation since it assumes at least a monotonic relationship, which is a assumption we don't want to make at this point.\n",
    "\n",
    "- Cramer's V for all combinations of categorical variables\n",
    "- Pearson and Spearman for all combinations of continuous variables\n",
    "\n",
    "- Mutal information for every combination of variables.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca34af-53ef-4d6d-bdf2-bed43cfc5070",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = credit_approval.variables[['name','type']].set_index('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b82f84-4c76-4bda-93ff-ec2810c16cc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Between categorical variables\n",
    "Cramer's V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580c104-d5c3-4e8f-9497-b543eefc5b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_cat    = category[category.type == 'Categorical'].index\n",
    "n_cat       = len(vars_cat)\n",
    "cramers_mat = pd.DataFrame(index=vars_cat, columns=vars_cat, dtype=np.float64)\n",
    "\n",
    "for var1, var2 in itertools.combinations(vars_cat, r=2):\n",
    "    cramers_mat.loc[var1,var2] = association(pd.crosstab(df[var1], df[var2]))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "axi = ax.matshow(cramers_mat)\n",
    "ax.set_xticks(range(n_cat), labels=vars_cat)\n",
    "ax.set_yticks(range(n_cat), labels=vars_cat)\n",
    "ax.set_xticklabels(labels=cramers_mat.columns, fontsize=30)\n",
    "ax.set_yticklabels(labels=cramers_mat.index, fontsize=30)\n",
    "\n",
    "cbar = fig.colorbar(axi, ax=ax)\n",
    "cbar.ax.tick_params(labelsize=30)\n",
    "plt.title('Cramer´s V \\n categorical data', fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2035aa1-fae0-4c61-8928-4058d2c1b500",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Between continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e8548-117b-444a-b078-ea184ffd6dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_con    = category[category.type == 'Continuous'].index\n",
    "n_con       = len(vars_con)\n",
    "\n",
    "pearson_mat = pd.DataFrame(index=vars_con, columns=vars_con, dtype=np.float64)\n",
    "spearman_mat = pd.DataFrame(index=vars_con, columns=vars_con, dtype=np.float64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c302be5-1b76-4be4-b3fd-9f69f8ae7a17",
   "metadata": {},
   "source": [
    "#### Pearson product-moment correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f9917-fbb0-4451-91e7-f3c3765ca0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var1, var2 in itertools.combinations(vars_con, r=2):\n",
    "    \n",
    "    pearson_mat.loc[var1,var2] =  pearsonr(\n",
    "        df[[var1,var2]].dropna(how='any')[var1],\n",
    "        df[[var1,var2]].dropna(how='any')[var2]).statistic\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "axi = ax.matshow(pearson_mat)\n",
    "ax.set_xticks(range(n_con), labels=vars_con)\n",
    "ax.set_yticks(range(n_con), labels=vars_con)\n",
    "ax.set_xticklabels(labels=pearson_mat.columns, fontsize=30)\n",
    "ax.set_yticklabels(labels=pearson_mat.index, fontsize=30)\n",
    "\n",
    "cbar = fig.colorbar(axi, ax=ax)\n",
    "cbar.ax.tick_params(labelsize=30)\n",
    "plt.title('Pearson product-moment correlation coefficient \\n continious data', fontsize=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fcd28c-1e91-4cb6-9bf9-42660cfcc66a",
   "metadata": {},
   "source": [
    "#### Spearman rank correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d07edd-dfa9-4935-a266-44238f27658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var1, var2 in itertools.combinations(vars_con, r=2):\n",
    "    \n",
    "    spearman_mat.loc[var1,var2] =  spearmanr(\n",
    "        df[[var1,var2]].dropna(how='any')[var1],\n",
    "        df[[var1,var2]].dropna(how='any')[var2]).statistic\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "axi = ax.matshow(spearman_mat)\n",
    "ax.set_xticks(range(n_con), labels=vars_con)\n",
    "ax.set_yticks(range(n_con), labels=vars_con)\n",
    "ax.set_xticklabels(labels=spearman_mat.columns, fontsize=30)\n",
    "ax.set_yticklabels(labels=spearman_mat.index, fontsize=30)\n",
    "\n",
    "cbar = fig.colorbar(axi, ax=ax)\n",
    "cbar.ax.tick_params(labelsize=30)\n",
    "plt.title('Spearman rank correlation coefficient \\n continious data', fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1db4f8-4be9-43a3-9450-cb29da6ee826",
   "metadata": {},
   "source": [
    "#### Mutual information (combinations of categorical and continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b08c8d2-eb5c-4bae-8dc5-f816d456225e",
   "metadata": {},
   "source": [
    "In this context, there is no reasonable way known to the authers to correlate a multi-class (k > 2) categorical variable to a continuous variable.\n",
    "\n",
    "- Therefore, we already do some tweaking of the data here, applying reasonable ways to allow us to correlate as many features as possible. Namely: Make A4, A5 and A13 binary variables by by summarising classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3cf03e-4f2b-4067-81f6-2efc90ee9cf1",
   "metadata": {},
   "source": [
    "- Also, we apply the log transform to features with a strongly skewed distribution (A3, A8, A11, A14, A15) already, to make the correlations more meaningfull, since we are not (!) doing rank correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443d27d7-4687-4968-b385-4b5f2a9c8ba2",
   "metadata": {},
   "source": [
    "- A obvious way to deal with A6 and A7 is target encoding, we will also already apply that here, for the sake of not leaving A6 and A7 completely out of the feature-feature correlations. This is however still EDA and not the feature engineering part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e62b6-7459-48d9-a393-3aacba4ddbcb",
   "metadata": {},
   "source": [
    "- Since only 5% of the data contains missing values, we will for now throw out any column that has missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0532c0a-5bf7-411d-8a84-decef5603c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_tweaker = make_column_transformer(\n",
    "    (\n",
    "        OneHotEncoder(sparse_output=False, drop='first'),\n",
    "        ['A1','A9','A10','A12','A16']\n",
    "    ),\n",
    "    (\n",
    "        FunctionTransformer(lambda col: label_binarize(col, classes=['u'])),\n",
    "        ['A4']\n",
    "    ),\n",
    "    (\n",
    "        FunctionTransformer(lambda col: label_binarize(col, classes=['g'])),\n",
    "        ['A5']\n",
    "    ),\n",
    "    (\n",
    "        # becoming numeric column\n",
    "        TargetEncoder(),\n",
    "        ['A6', 'A7']\n",
    "    ),\n",
    "    (\n",
    "        FunctionTransformer(lambda col: label_binarize(col, classes=['g'])),\n",
    "        ['A13']\n",
    "    ),\n",
    "    (\n",
    "        FunctionTransformer(np.log1p),\n",
    "        ['A2','A3','A8','A11','A14','A15']\n",
    "    ),\n",
    "\n",
    "    # leave everything else untouched\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "df_tweaked = df.copy(deep=True).dropna(how='any')\n",
    "var_names = df.columns\n",
    "\n",
    "df_tweaked = pd.DataFrame(column_tweaker.fit_transform(df_tweaked, y=df_tweaked.A16), columns=['A1','A9','A10','A12' ,'A16','A4','A5','A6','A7','A13','A2','A3','A8','A11','A14','A15'])\n",
    "df_tweaked = df_tweaked[var_names] # bring columns in right order again\n",
    "\n",
    "category = credit_approval.variables[['name','type']].set_index('name')\n",
    "category.loc[['A6','A7'],:] = 'Continuous'\n",
    "\n",
    "df_tweaked.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0841b7-dbcd-4eaf-95e1-26ce95572842",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore', UserWarning)\n",
    "\n",
    "mutual_informaion = pd.DataFrame(index=var_names, columns=var_names)\n",
    "\n",
    "# each variable is taken as target variable\n",
    "for var in var_names:\n",
    "    \n",
    "    discrete_features_mask = (category.loc[category.index!=var,:] == 'Categorical').values.ravel()\n",
    "    \n",
    "    # use mutual_info_regression if targeted variable is continuous\n",
    "    if category.loc[var][0] == 'Continuous':\n",
    "        mutual_informaion.loc[var, var_names != var] = mutual_info_regression(\n",
    "            df_tweaked.loc[:, df_tweaked.columns != var],\n",
    "            df_tweaked[var],\n",
    "            discrete_features=discrete_features_mask\n",
    "        )\n",
    "\n",
    "    # use mutual_info_regression if targeted variable is categorical\n",
    "    if category.loc[var][0] == 'Categorical':\n",
    "        mutual_informaion.loc[var, var_names != var] = mutual_info_classif(\n",
    "            df_tweaked.loc[:, df_tweaked.columns != var],\n",
    "            df_tweaked[var],\n",
    "            discrete_features=discrete_features_mask\n",
    "        )\n",
    "mutual_informaion = mutual_informaion.fillna(0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "axi = ax.matshow(mutual_informaion)\n",
    "ax.set_xticks(range(len(var_names)), labels=var_names)\n",
    "ax.set_yticks(range(len(var_names)), labels=var_names)\n",
    "\n",
    "#plt.set_title('mutual information')\n",
    "plt.colorbar(axi, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce96452-f724-43fa-a24c-1295765ed6e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Determining clustering for A7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e416e5e-7ac2-4648-a735-3a65e1a5836a",
   "metadata": {},
   "source": [
    "Principal component analysis allows us to project data into a 2D space, where we can visualise it.\n",
    "\n",
    "We wanted to summarize categories for A7, since we don't have any semantic information on the categories but still want a way to figure out, which categories to group together we can try to utilise PCA.\n",
    "\n",
    "\n",
    "If we project the data, we can visually inspect, if there are any clusters which suggest a possible grouping of A7's categories.\n",
    "\n",
    "We try PCA for differet sets of features and selectively in combination with the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10080ac5-337a-42ec-b833-75353db114df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Set up PCA for projection into 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e071a1-e11a-4534-a288-ae081773a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tweaked = df_tweaked.iloc[:,:15]\n",
    "y_tweaked = pd.DataFrame(df_tweaked.A16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e51ef5-f49b-4797-b431-fe0ea3d9567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tweaked_train, X_tweaked_test, y_tweaked_train, y_tweaked_test = train_test_split(X_tweaked, y_tweaked, test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e04167-4104-48fb-92e1-67ec09e2ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    PCA(n_components=2,svd_solver='full')\n",
    ")\n",
    "pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825aca44-e5b4-4a9b-9136-6cd9ae0410b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af91ae77-bb4e-46ae-bf8d-72c220427428",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(pca.fit_transform(df_tweaked), columns = ['x','y'])\n",
    "sns.scatterplot(res, x='x', y='y', hue=df.A7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a05b163-e177-405c-aa48-df2d49e6349a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Feature space only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b85fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tweaked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb437fba-d991-471b-be93-aa67678b71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(pca.fit_transform(X_tweaked), columns = ['x','y'])\n",
    "sns.scatterplot(res, x='x', y='y', hue=df.A7).set(title='PCA A7 \\n inklusive target')\n",
    "plt.legend(bbox_to_anchor=(0.67, 0.5), ncol=2, loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec3fd9-c386-4f8e-9414-8f809ba739ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### exclude A7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8420563-c453-4f1d-8b74-e6fd9407d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a7=df_tweaked.loc[:,df_tweaked.columns!='A7']\n",
    "\n",
    "res = pd.DataFrame(pca.fit_transform(df_a7), columns = ['x','y'])\n",
    "sns.scatterplot(res, x='x', y='y', hue=df.A7).set(title='PCA A7 \\n exklusive target')\n",
    "plt.legend(bbox_to_anchor=(0.67, 0.55), ncol=2, loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbbcbe1-45a4-45d8-924d-26c9052c84b0",
   "metadata": {},
   "source": [
    "#### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376d027a-efa8-4516-b853-68f084310807",
   "metadata": {},
   "source": [
    "There are no obvious visible clusters resulting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c051a7b-063c-4e4d-8543-9413ba08faef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Feature space PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f1a9e-f917-4563-816b-06bb28ead9b6",
   "metadata": {},
   "source": [
    "To further improve our understanding of the dataset, we again perform a PCA.\n",
    "\n",
    "This time we want to look into how many principal components our feature space will be decomposed and how much varaince each of these components is explaining.\n",
    "\n",
    "This is done to get a idea of the dimensionality of the data, which will further our understanding of the dataset, will be usefull when deciding how much PCA to perform during feature engingeering and to already get a idea of what classifiers we might use later on, as different classifiers show different effectivity with high or low dimensional data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f7cb75-bd06-4654-8ddb-95fbcf6809bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    PCA(svd_solver='full')\n",
    ")\n",
    "pca\n",
    "\n",
    "res = pca.fit(X_tweaked).transform(X_tweaked)\n",
    "\n",
    "plt.bar(range(len(pca['pca'].explained_variance_ratio_)), pca['pca'].explained_variance_ratio_)\n",
    "plt.title('Explained variance ratio of individual components');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ec13c-ab4d-42ff-8041-1c1c97a4a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(pca['pca'].explained_variance_ratio_)), np.cumsum(pca['pca'].explained_variance_ratio_))\n",
    "plt.title('Cumulative explained variance ratio of components');\n",
    "plt.grid();\n",
    "np.round(np.cumsum(pca['pca'].explained_variance_ratio_),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2d61a5-16f2-4887-897a-b6a8ee0ca2d0",
   "metadata": {},
   "source": [
    "We already explain 80% of the variance with 4 components and around 99% with 12 components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8a0ba-8d47-4e90-9f19-a92a91546ef9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235739e8-0af0-4ae0-ac8c-73237b033494",
   "metadata": {},
   "source": [
    "### A4 & A5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df3b12e-274d-4209-b0cc-4ee31f5ee266",
   "metadata": {},
   "source": [
    "A4 and A5 have a correlation coefficient of 1. Therefore they are identical and one of them can instantly be discarded from the dataset (we discard A5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f48fb50-bf85-45e3-ac17-35b9cdf2e00f",
   "metadata": {},
   "source": [
    "### A7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65d856-288c-49a0-b8e0-d0320807c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "cramers_mat.loc['A7','A6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876cefb-e9b5-43ae-8d16-b6db3f49f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_informaion.loc['A7','A6']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17761652-9a2a-4d92-8130-389f21c8c680",
   "metadata": {},
   "source": [
    "How to deal with A7 ?\n",
    "\n",
    "A categorical feature with 9 categories. Target encoding didn't bring very promising univariate results even after summarizing from 9 to 3 categories.\n",
    "\n",
    "\n",
    "However, we also found A7 to only have a medium correlation with the target variable in comparison to the other features and shares a relatively high ammount of mutual information with A6.\n",
    "\n",
    "Therefore it seems like a reasonable option either just keep the not very promising looking target encoding, as to only add one dimension through this feature, or leave it out all together (which feature selection should do automaticly if our univariate observations also apply in combination with the other features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659ddf6-80e7-4cc5-8bab-539fd278878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat = pd.DataFrame(index=df.columns, columns=['tweaking','imputation'], data= \" \")\n",
    "\n",
    "strat.loc['A1','tweaking'] = 'OneHotEncoding'\n",
    "strat.loc['A1','imputation'] = 'MostFrequent category'\n",
    "\n",
    "#strat.loc['A1','tweaking'] = 'OneHotEncoding'\n",
    "\n",
    "strat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
